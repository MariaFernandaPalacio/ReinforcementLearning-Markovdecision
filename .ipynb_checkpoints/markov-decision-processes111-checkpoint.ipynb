{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpmFfXsQ0dYI"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp>\n",
    "                <h1 style=\"color:blue;text-align:center\">Aprendizaje por refuerzo</h1\n",
    "            </tp>\n",
    "            <tp>\n",
    "                <p style=\"font-size:150%;text-align:center\">Procesos de decisión de Markov y Programación dinámica</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3SkDSWJ0dYJ"
   },
   "source": [
    "# Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook veremos una manera de implementar los ambientes de tarea de los MDP. Esta implementación sigue el formato de los entornos de [gym](https://gymnasium.farama.org). También implementaremos los algoritmos de Programación dinámica para la evaluación y mejoramiento de políticas.\n",
    "\n",
    "Este notebook está basado en las presentación de Sanghi (2021), capítulo 2 y sus [notebooks](https://github.com/Apress/deep-reinforcement-learning-python); Sutton R., & Barto, A., (2015), capítulos 3 y 4; y también Winder, P., (2021), capítulo 2 y su [notebook](https://rl-book.com/learn/mdp/code_driven_intro/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir a ejercicio 1](#ej1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias\n",
    "\n",
    "Al iniciar el notebook o reiniciar el kerner, se pueden cargar todas las dependencias de este notebook al correr las siguientes celdas. Este también es el lugar para instalar las dependencias faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym[all]\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Del notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tiempos import *\n",
    "from ambientes import *\n",
    "from agentes import Agent\n",
    "from utils import Episode, PlotGridValues\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnQ_gA70dYL"
   },
   "source": [
    "# Secciones\n",
    "\n",
    "Desarrollaremos la explicación de la siguiente manera:\n",
    "\n",
    "1. [Ejemplos de implementación](#impl).\n",
    "    1. [ABC](#abc).\n",
    "    2. [GridWorld](#gw).\n",
    "    3. [Frozen Lake](#frozen).\n",
    "2. [Evaluación de políticas](#poli-eval).\n",
    "3. [Mejoramiento de políticas](#dp).\n",
    "    1. [Policy iteration](#poli-iter).\n",
    "    2. [Value iteration](#value-iter).\n",
    "    3. [Resolviendo el Frozen Lake](#fl).\n",
    "    3. [Comparación de tiempos](#comp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4x5lJfd0dYM"
   },
   "source": [
    "# Ejemplos de implementación <a class=\"anchor\" id=\"impl\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Existen muchísimas aplicaciones muy interesantes de RL. Ellas requieren ambientes relativamente complejos, incluso simulaciones del mundo físico en 3D. En este momento existen muchos ambientes ya implementados en python y de acceso libre, que han sido compilados en los [ambientes de gym](https://gymnasium.farama.org) de la empresa OpenAI. Vamos a familiarnos un poco con la implementación particular propuesta por esta librería.\n",
    "\n",
    "Comenzaremos con la implementación de un ejemplo muy usado para visualizar valores de estados y políticas, que es el ejemplo del Grid World. Luego, mostraremos la implementación del problema del Frozen Lake, el cual es un ejemplo de un entorno tomado de Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABC <a class=\"anchor\" id=\"abc\"></a>\n",
    "\n",
    "([Volver a Ejemplos](#impl))\n",
    "\n",
    "Un entorno muy sencillo que hemos trabajado en clase es el ABC:\n",
    "\n",
    "<img src=\"./imagenes/abc.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = ABC()\n",
    "print(\"Acciones posibles (valores):\", env.action_space)\n",
    "print(\"Acciones posibles (nombres):\", np.vectorize(env.dict_acciones.get)(env.action_space))\n",
    "print('')\n",
    "print(\"Número de acciones posibles:\", len(env.action_space))\n",
    "print('')\n",
    "a = random.choice(env.action_space)\n",
    "print(\"Una acción posible seleccionada al azar:\", env.dict_acciones[a])\n",
    "print('')\n",
    "print(\"El estado actual es:\", env.state, \"que corresponde a\", env.dict_states[env.state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al imprimir el objeto podemos ver el modelo del MDP que ha sido implementado mediante el método `_transition_prob`, el cual define, para cada estado y acción, la siguiente tupla: \n",
    "\n",
    "(probabilidad, próximo estado, recompensa, finalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World <a class=\"anchor\" id=\"gw\"></a>\n",
    "\n",
    "([Volver a Ejemplos](#impl))\n",
    "\n",
    "Un ejemplo muy útil en términos de visualización de MDP es el del Grid World, el cual consiste de una rejilla rectangular. Las casillas corresponden a los estados. Hay cuatro acciones: norte, sur, este y oeste, que hacen que el agente se mueva una casilla en la dirección respectiva en la rejilla. Las acciones que sacarían al agente de la rejilla dejan su ubicación sin cambios. Cada acción da como resultado una recompensa de -1. Las casillas (0,0) y (4,4) son estados terminales.\n",
    "\n",
    "Veamos una implementación *ad hoc* tomada del libro de Sanghi (ver [código](https://github.com/Apress/deep-reinforcement-learning-python/blob/main/chapter3/gridworld.py)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos algunas características de la clase `GridworldEnv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "print(\"Acciones posibles (valores):\", env.action_space)\n",
    "print(\"Acciones posibles (nombres):\", np.vectorize(env.dict_acciones.get)(env.action_space))\n",
    "print('')\n",
    "print(\"Número de acciones posibles:\", len(env.action_space))\n",
    "print('')\n",
    "a = random.choice(env.action_space)\n",
    "print(\"Una acción posible seleccionada al azar:\", env.dict_acciones[a])\n",
    "print('')\n",
    "print(\"El estado actual es:\", env.state, \"que corresponde a la casilla\", np.unravel_index(env.state, shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al imprimir el objeto podemos ver el modelo del MDP que ha sido implementado mediante el método `_transition_prob`, el cual define, para cada estado y acción, la siguiente tupla: \n",
    "\n",
    "(probabilidad, próximo estado, recompensa, finalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los métodos más importantes de la clase es el `step()`, el cual recibe una acción como argumento y, junto con la información del estado actual y el modelo de transiciones, obtiene el estado al que pasa el sistema y devuelve una recompensa. También se obtiene un valor booleano que indica si el estado obtenido es terminal o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()\n",
    "obs, reward, done = env.step(env.SOUTH)\n",
    "print(f'Estado={obs} at {np.unravel_index(env.state, env.shape)}, Recompensa={reward}, Finalizado={done}')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej1\"></a>**Ejercicio 1:** \n",
    "\n",
    "([Próximo ejercicio](#ej2))\n",
    "\n",
    "Cree una pequeña función para hacer una caminata aleatoria por la rejilla hasta que el agente llegue a un estado terminal. Encuentre la utlidad del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen lake <a class=\"anchor\" id=\"frozen\"></a>\n",
    "\n",
    "([Volver a Ejemplos](#impl))\n",
    "\n",
    "Ahora veremos un entorno de gymnasium que se llama [fozen lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/). El agente debe cruzar un lago congelado de principio a fin sin caer en ningún agujero al caminar. Es posible que el jugador no siempre se mueva en la dirección deseada debido a la naturaleza resbaladiza del lago congelado.\n",
    "\n",
    "El código fuente del entorno puede encontrarse en: [source code](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/toy_text/frozen_lake.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slippery = True  # Make False to avoid slipping\n",
    "env = gym.make('FrozenLake-v1', \n",
    "               desc=None, \n",
    "               map_name=\"4x4\",\n",
    "               is_slippery=is_slippery, \n",
    "               render_mode='rgb_array')\n",
    "obs = env.reset()\n",
    "print(\"Observacion inicial: \", obs)\n",
    "print(\"Número de acciones posibles:\", env.action_space.n)\n",
    "print(\"Acciones posibles:\", env.action_space)\n",
    "print(\"Una acción posible seleccionada al azar:\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el anterior ambiente, podemos acceder a las probabilidades de transición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos algunas acciones aleatorias del agente en el entorno. Esta vez usamos la clase `Episode` del módulo `utils` y el agente `Agent` del módulo `agentes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slippery = False\n",
    "size = 4\n",
    "max_rounds = 50\n",
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', \n",
    "               desc=None, \n",
    "               map_name=\"4x4\",\n",
    "               is_slippery=is_slippery, \n",
    "               render_mode='rgb_array'\n",
    "              )\n",
    "env = TimeLimit(env, max_episode_steps=max_rounds)\n",
    "# Create agent\n",
    "parameters = {\n",
    "    \"nS\": size*size,\n",
    "    \"nA\": env.action_space.n,\n",
    "    \"gamma\":0.99,\n",
    "    \"epsilon\":0\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=agent,\\\n",
    "        model_name='Random',\\\n",
    "        num_rounds=15)\n",
    "# Visualize\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de políticas <a class=\"anchor\" id=\"poli-eval\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Vamos a usar la programación dinámica para encontrar los valores de estado para una política dada. La idea central es usar la ecuación de Bellman como una regla iterativa:\n",
    "\n",
    "$$v_{k+1}(s) = \\sum_{s'}\\left( p(s' | s,\\pi(s)) \\Bigl[ r + \\gamma v_k(s') \\Bigr] \\right)$$\n",
    "\n",
    "Esto da lugar al siguiente algoritmo:\n",
    "\n",
    "<img src=\"./imagenes/policy_evaluation.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej2\"></a>**Ejercicio 2:** \n",
    "\n",
    "([Anterior ejercicio](#ej1)) ([Próximo ejercicio](#ej3))\n",
    "\n",
    "Vamos a encontrar los valores de estado para el problema del [ABC](#abc). Implemente el algoritmo iterativo de evaluación de políticas y utilícelo para encontrar los valores de la política `policy`, de acuerdo a la cual el agente se mueve a la derecha en cualquier estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy evaluation\n",
    "\n",
    "def policy_eval(env, policy, discount_factor=1.0, theta=0.01, verbose=False):\n",
    "    \"\"\"\n",
    "    Evalúa una política para un entorno.\n",
    "    Input:\n",
    "        - env: transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - policy: vector de longitud env.nS que representa la política\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "        - verbose: 0 no imprime nada, \n",
    "                   1 imprime la iteración del valor\n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la función de valor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def expected_value(s, V):\n",
    "        # Calcular el valor esperado as per backup diagram\n",
    "        value = 0\n",
    "        a = policy[s]\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            # AQUÍ SU CÓDIGO\n",
    "            value += ... # la fórmula de valor esperado\n",
    "            # AQUÍ TERMINA SU CÓDIGO\n",
    "        return value\n",
    "    \n",
    "    # Start with a (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    # Stop if change is below a threshold\n",
    "    continuar = True\n",
    "    while continuar:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            # AQUÍ SU CÓDIGO\n",
    "            v = ... # el valor guardado para s\n",
    "            V[s] = ... # el nuevo valor en términos del expected_value\n",
    "            delta = ... # la diferencia del nuevo valor con el valor guardado\n",
    "            # AQUÍ TERMINA SU CÓDIGO\n",
    "            if verbose:\n",
    "                print(f'V[{env.dict_states[s]}]={round(V[s], 3)}; ', end='')\n",
    "        continuar = not delta < theta\n",
    "        print('')\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ABC()\n",
    "policy = [env.RIGHT, env.RIGHT, env.RIGHT]\n",
    "V = policy_eval(env, \n",
    "                policy, \n",
    "                discount_factor=0.8, \n",
    "                theta=0.01, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** El resultado de debe ser \n",
    "```\n",
    "V[A]=-1.0; V[B]=8.9; V[C]=0.0; \n",
    "V[A]=5.328; V[B]=9.612; V[C]=0.0; \n",
    "V[A]=6.347; V[B]=9.669; V[C]=0.0; \n",
    "V[A]=6.469; V[B]=9.674; V[C]=0.0; \n",
    "V[A]=6.482; V[B]=9.674; V[C]=0.0; \n",
    "V[A]=6.484; V[B]=9.674; V[C]=0.0; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej3\"></a>**Ejercicio 3:** \n",
    "\n",
    "([Anterior ejercicio](#ej2)) ([Próximo ejercicio](#ej4))\n",
    "\n",
    "Vamos a encontrar los valores de estado para el entorno del GridWorld. Usaremos la política `policy` definida en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "policy = [env.NORTH, env.EAST, env.EAST, env.SOUTH] * 4\n",
    "pp = PlotGridValues(shape=shape, action_dict=env.dict_acciones)\n",
    "pp.plot_policy(np.reshape(policy, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3\n",
    "V = ... # aquí el código para correr policy_eval\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "pp.plot_policy(np.reshape(policy, shape), ax=ax[0])\n",
    "pp.plot_V_values(V=np.reshape(V, shape), ax=ax[1])\n",
    "ax[0].set_title('Policy', fontsize='18')\n",
    "ax[1].set_title('Values', fontsize='18')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejoramiento de políticas <a class=\"anchor\" id=\"dp\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Recordemos que el propósito del RL es encontrar la acción que mejor utilidad tenga en cada estado, determinando así la política óptima para el problema. Si conocemos el modelo de un MDP, podemos ir mejorando una política paso a paso. Los dos métodos de esta sección realizan el mejoramiento de una política hasta llegar a la política óptima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration <a class=\"anchor\" id=\"poli-iter\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "En este algoritmo se busca mejorar una política $\\pi$ en cada estado $s$, definiendo $\\pi'$ de tal manera que:\n",
    "\n",
    "$$\\pi'(s) = \\mbox{arg}\\max_a q_{\\pi}(s,a)$$\n",
    "\n",
    "Esto da lugar a una nueva política $\\pi'$. Luego, se recalculan los valores $v_{\\pi'}(s)$ usando el algoritmo de evaluación de política visto en la sección anterior. Este proceso se itera hasta converger a la política óptima:\n",
    "\n",
    "<img src=\"./imagenes/p_i.png\" width=\"350\"/>\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "<img src=\"./imagenes/policy_iteration1.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej4\"></a>**Ejercicio 4:** \n",
    "\n",
    "([Anterior ejercicio](#ej3)) ([Próximo ejercicio](#ej5))\n",
    "\n",
    "Implemente el algoritmo de policy improvement y mejore la política del ejercicio 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement\n",
    "\n",
    "def policy_iteration(env, pol, discount_factor=1.0, theta=0.01):      \n",
    "    \"\"\"\n",
    "    Mejoramiento de una política.\n",
    "    Input:\n",
    "        - env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - pol: vector de longitud env.nS que representa la política\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "        - verbose: bool to print intermediate values. \n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la política óptima.\n",
    "    \"\"\"     \n",
    "    def expected_value(s, a, env, V):\n",
    "        # Calcular el valor esperado as per backup diagram\n",
    "        value = 0\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            # Aquí su código\n",
    "            value += ... # expected value\n",
    "            # Hasta aquí su código\n",
    "        return value\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = deepcopy(pol)\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        # Aquí su código\n",
    "        V = ... # policy evaluation\n",
    "        policy_stable = True\n",
    "        for s in range(env.nS):\n",
    "            a_ = ... # action predicted by policy at state s\n",
    "            policy[s] = ... # argmax of expected values\n",
    "            if a != policy[s]:\n",
    "                policy_stable = ... # Boolean to check if policy is the same\n",
    "        # Hasta aquí su código\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "pp = PlotGridValues(shape=shape, action_dict=env.dict_acciones)\n",
    "policy = ([env.NORTH] + [env.EAST]* (shape[0]-2) + [env.SOUTH]) * shape[1]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "pp.plot_policy(np.reshape(policy, shape), ax=ax[0])\n",
    "policy, V = policy_iteration(env, policy, discount_factor=1, theta=0.01)\n",
    "pp.plot_policy(np.reshape(policy, shape), V=V.reshape(shape), ax=ax[1])\n",
    "ax[0].set_title('Initial policy', fontsize='18')\n",
    "ax[1].set_title('Final policy', fontsize='18')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** La respuesta debe ser:\n",
    "\n",
    "<img src=\"./imagenes/pol_it.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar el comportamiento del agente con esta política:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "# Create agent\n",
    "parameters = {\n",
    "    \"nS\": size*size,\n",
    "    \"nA\": env.nA,\n",
    "    \"gamma\":0.99,\n",
    "    \"epsilon\":0\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "stochastic_policy = {(s, a):1 if policy[s]==a else 0 for a in range(env.nA) for s in range(env.nS)}\n",
    "agent.policy = stochastic_policy\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=agent,\\\n",
    "        model_name='DP',\\\n",
    "        num_rounds=15)\n",
    "# Visualize\n",
    "episodio.sleep_time = 0.5\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration <a class=\"anchor\" id=\"value-iter\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "Para mejorar el desempeño del algoritmo de policy iteration, se puede truncar la evaluación de la política después de una iteración para cada estado. Además, se puede combinar, en una sola regla iterativa, el mejoramiento de la política con la evaluación truncada de la política:\n",
    "\n",
    "$$v_{k+1}(s) = \\max_{a}\\sum_{s'}\\left( p(s' | s, a) \\Bigl[ r + \\gamma  v_{k}(s') \\Bigr] \\right)$$\n",
    "\n",
    "Se puede demostrar que la sucesión $\\{v_k\\}$ converge a $v_*$. \n",
    "\n",
    "<img src=\"./imagenes/diagrama_value_iteration.png\" width=\"250\"/>\n",
    "\n",
    "Finalmente, para obtener la política óptima $\\pi_*$ se buscan las acciones mediante el argmax de los valores óptimos obtenidos en el proceso anterior:\n",
    "\n",
    "$$\\pi_*(s) = \\mbox{arg}\\max_a\\sum_{s'}\\left( p(s' | s, a) \\Bigl[ r + \\gamma  v_{*}(s') \\Bigr] \\right)$$\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "<img src=\"./imagenes/value_iteration.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej5\"></a>**Ejercicio 5:** \n",
    "\n",
    "([Anterior ejercicio](#ej4)) ([Próximo ejercicio](#ej6))\n",
    "\n",
    "Implemente el algoritmo de value-iteration para encontrar la política óptima del MDP. Use su algoritmo para encontrar la política óptima del Grid World. El resultado debe ser la misma política óptima encontrada mediante el método `policy_iteration` en el ejercicio 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 5\n",
    "# Value iteration\n",
    "def value_iteration(env, discount_factor=1.0, theta=0.01):\n",
    "    \"\"\"\n",
    "    Mejoramiento de una política.\n",
    "    Input:\n",
    "        - env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la política óptima.\n",
    "    \"\"\" \n",
    "    pass\n",
    "    def expected_value(s, a, env, V):\n",
    "        value = 0\n",
    "        # Calcular el valor esperado as per backup diagram\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            # Aquí su código\n",
    "            value += ... # expected value\n",
    "            # Hasta aquí su código\n",
    "        return value\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    continuar = True\n",
    "    while continuar:\n",
    "        # Aquí su código\n",
    "        delta = ... # initialize delta\n",
    "        for s in range(env.nS):\n",
    "            v = ... # previous stored value\n",
    "            V[s] = ... # new value is max expected value\n",
    "            delta = ... # max between previous and new value \n",
    "        continuar = not delta < theta\n",
    "    # Update policy\n",
    "    policy = [np.argmax([expected_value(s, a, env, V) for a in range(env.nA)]) for s in range(env.nS)]\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "policy, V = value_iteration(env, discount_factor=1, theta=0.01)\n",
    "pp = PlotGridValues(shape=shape, action_dict=env.dict_acciones)\n",
    "pp.plot_policy(np.reshape(policy, shape), V=np.reshape(V, shape))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolviendo el Frozen Lake\n",
    "\n",
    "Ya tenemos las herramientas para resolver el entorno del Frozen Lake. En este entorno, el agente debe caminar por el hielo para recoger el regalo que se encuentra al otro lado sin caerse en ninguno de los huecos en el hielo. El problema es que el hielo es resbaladizo y el agente puede resbalarse y moverse hacia alguno de los lados con probabilidad 1/3. Veámos cuál es la política óptima con un solo hueco en el hielo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "is_slippery = True\n",
    "size = 4\n",
    "max_rounds = 50\n",
    "env = gym.make('FrozenLake-v1', \n",
    "               desc=[\"SFFF\", \"FFHF\", \"FFFF\", \"FFFG\"], \n",
    "               is_slippery=is_slippery, \n",
    "               render_mode='rgb_array'\n",
    "              )\n",
    "env = TimeLimit(env, max_episode_steps=max_rounds)\n",
    "env.reset()\n",
    "dict_acciones = {3:\"⬆\", 2:\"➡\", 1:\"⬇\", 0:\"⬅\"}\n",
    "dict_states = dict(zip([s for s in range(16)], [np.unravel_index(s, (4,4)) for s in range(16)]))\n",
    "setattr(env, 'nS', 16)\n",
    "setattr(env, 'nA', 4)\n",
    "setattr(env, 'dict_acciones', dict_acciones)\n",
    "setattr(env, 'dict_states', dict_states)\n",
    "# Usamos value iteration para encontrar política óptima\n",
    "policy, V = value_iteration(env,\n",
    "                         discount_factor=0.9, \n",
    "                         theta=0.01)\n",
    "pp = PlotGridValues(shape=shape, action_dict=env.dict_acciones)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "ax[0].set_title('Environment', fontsize='18')\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].imshow(env.render())\n",
    "pp.plot_policy(np.reshape(policy, shape), V=np.reshape(V, shape), ax=ax[1])\n",
    "ax[1].set_title('Best policy', fontsize='18')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante observar que no hemos resuelto el problema mediante `policy_iteration`, sino mediante `value_iteration` debido a los tiempos de computación. Para ver detalles de la comparación entre los tiempos de los dos algoritmos, puede consultar la sección [Comparación de tiempos](#comp).\n",
    "\n",
    "Veámos al agente en acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slippery = True\n",
    "size = 4\n",
    "max_rounds = 50\n",
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', \n",
    "               desc=[\"SFFF\", \"FFHF\", \"FFFF\", \"FFFG\"], \n",
    "#               map_name=\"4x4\",\n",
    "               is_slippery=is_slippery, \n",
    "               render_mode='rgb_array'\n",
    "              )\n",
    "env = TimeLimit(env, max_episode_steps=max_rounds)\n",
    "dict_acciones = {3:\"⬆\", 2:\"➡\", 1:\"⬇\", 0:\"⬅\"}\n",
    "dict_states = dict(zip([s for s in range(16)], [np.unravel_index(s, (4,4)) for s in range(16)]))\n",
    "setattr(env, 'nS', 16)\n",
    "setattr(env, 'nA', 4)\n",
    "setattr(env, 'dict_acciones', dict_acciones)\n",
    "setattr(env, 'dict_states', dict_states)\n",
    "# Create agent\n",
    "parameters = {\n",
    "    \"nS\": size*size,\n",
    "    \"nA\": env.nA,\n",
    "    \"gamma\":0.9,\n",
    "    \"epsilon\":0\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "stochastic_policy = {(s, a):1 if policy[s]==a else 0 for a in range(env.nA) for s in range(env.nS)}\n",
    "agent.policy = stochastic_policy\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=agent,\\\n",
    "        model_name='DP',\\\n",
    "        num_rounds=50)\n",
    "# Visualize\n",
    "episodio.sleep_time = 0.5\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de tiempos <a class=\"anchor\" id=\"comp\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "Vamos a hacer el estudio empírico de la complejidad de tiempos de los dos algoritmos. Correremos ambos algoritmos sobre el Grid World con tamaños (4,4) hasta (9,9). Con cada ambiente correremos 10 cada algoritmo y registraremos los tiempos de máquina usados para encontrar la política óptima. Los resultados son los siguientes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = lambda env: policy_iteration(env, get_nice_policy(env.shape))\n",
    "v_i = lambda env: value_iteration(env)\n",
    "funs = [p_i, v_i]\n",
    "nombres_funs = ['policy-iteration', 'value-iteration']\n",
    "shapes = [(n,n) for n in range(4,15)]\n",
    "lista_args = [GridworldEnv(shape) for shape in shapes]\n",
    "df = compara_entradas_funs(funs, nombres_funs, lista_args, N=10)\n",
    "sns.lineplot(x='Long_entrada',y='Tiempo',hue='Funcion',data=df)\n",
    "plt.savefig('figura.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gráfica muestra el tiempo promedio que cada algoritmo toma para encontrar la política óptima con distintos tamaños del Grid World. A partir de la gráfica queda muy claro que el algoritmo de `value-iteration` es más eficiente que el de `policy-iteration`. Observe también que el primero no necesita una política de entrada, mientras que sí se requiere una política como argumento del `policy-iteration`. En este ejemplo se tomó una política que converge relativamente rápido, pero otras políticas toman muchísimo más tiempo en converger. Todo esto muestra las ventajas del `value-iteration`, el cual es más rápido y no requiere política de entrada.\n",
    "\n",
    "Adicionalmente, observe que la complejidad de tiempo va creciendo mucho en ambos casos, lo cual hace que sean inviables para MDPs que tienen una gran cantidad de estados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En este notebook usted aprendió\n",
    "\n",
    "* Cómo implementar MDP en python usando la librería `gym` de OpenAI.\n",
    "* Cómo implementar la evaluación de una política, para obtener los valores en cada estado.\n",
    "* Cómo implementar la metodología de mejoramiento de políticas mediante policy iteration y value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Shanghi, N. (2021) Deep Reinforcement Learning with Python: With PyTorch, TensorFlow and OpenAI Gym. Apress. \n",
    "\n",
    "Sutton R., & Barto, A., (2015) Reinforcement Learning: An Introduction, 2nd Edition. A Bradford Book. Series: Adaptive Computation and Machine Learning series. \n",
    "\n",
    "Winder, P., (2021) Reinforcement Learning: Industrial Applications of Intelligent Agents. O’Relly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "arboles_busqueda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
